<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <link rel="stylesheet" href="/ovn-kubernetes/assets/css/style.css?v=9e6c62f21184b23c5bdbecd995abe5f149115b0f" media="screen" type="text/css">
    <link rel="stylesheet" href="/ovn-kubernetes/assets/css/print.css" media="print" type="text/css">

    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>OVS Hardware Offload | OVN-Kubernetes Homepage</title>
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="OVS Hardware Offload" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="test" />
<meta property="og:description" content="test" />
<link rel="canonical" href="http://www.ovn.org/ovn-kubernetes/ovs_offload.html" />
<meta property="og:url" content="http://www.ovn.org/ovn-kubernetes/ovs_offload.html" />
<meta property="og:site_name" content="OVN-Kubernetes Homepage" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="OVS Hardware Offload" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"test","headline":"OVS Hardware Offload","url":"http://www.ovn.org/ovn-kubernetes/ovs_offload.html"}</script>
<!-- End Jekyll SEO tag -->


    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->
<!-- link rel="shortcut icon" type="image/x-icon" href="/ovn-kubernetes/favicon.ico" -->

<!-- end custom head snippets -->

  </head>

  <body>
    <header>
      <div class="inner">
        <a href="http://www.ovn.org/ovn-kubernetes/">
          <h1>OVN-Kubernetes Homepage</h1>
        </a>
        <h2>test</h2>
        
          <a href="https://github.com/ovn-org/ovn-kubernetes" class="button"><small>View project on</small> GitHub</a>
        
        
      </div>
    </header>

    <div id="content-wrapper">
      <div class="inner clearfix">
        <section id="main-content">
          <h1 id="ovs-hardware-offload">OVS Hardware Offload</h1>

<p>The OVS software based solution is CPU intensive, affecting system performance
and preventing fully utilizing available bandwidth. OVS 2.8 and above support
new feature called OVS Hardware Offload which improves performance significantly. 
This feature allows to offload the OVS data-plane to the NIC while maintaining 
OVS control-plane unmodified. It is using SR-IOV technology with VF representor
host net-device. The VF representor plays the same role as TAP devices
in Para-Virtual (PV) setup. A packet sent through the VF representor on the host
arrives to the VF, and a packet sent through the VF is received by its representor.</p>

<h2 id="supported-ethernet-controllers">Supported Ethernet controllers</h2>

<p>The following manufacturers are known to work:</p>

<ul>
  <li>Mellanox ConnectX-5 NIC</li>
  <li>Mellanox ConnectX-6DX NIC</li>
</ul>

<h2 id="prerequisites">Prerequisites</h2>

<ul>
  <li>Linux Kernel 5.7.0 or above</li>
  <li>Open vSwitch 2.13 or above</li>
  <li>iproute &gt;= 4.12</li>
  <li>sriov-device-plugin</li>
  <li>multus-cni</li>
</ul>

<h2 id="worker-node-sr-iov-configuration">Worker Node SR-IOV Configuration</h2>

<p>In order to enable Open vSwitch hardware offloading, the following steps
are required. Please make sure you have root privileges to run the commands
below.</p>

<p>Check the Number of VF Supported on the NIC</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cat /sys/class/net/enp3s0f0/device/sriov_totalvfs
8
</code></pre></div></div>

<p>Create the VFs</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>echo '4' &gt; /sys/class/net/enp3s0f0/device/sriov_numvfs
</code></pre></div></div>

<p>Verfiy the VFs are created</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ip link show enp3s0f0
8: enp3s0f0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP mode DEFAULT qlen 1000
   link/ether a0:36:9f:8f:3f:b8 brd ff:ff:ff:ff:ff:ff
   vf 0 MAC 00:00:00:00:00:00, spoof checking on, link-state auto
   vf 1 MAC 00:00:00:00:00:00, spoof checking on, link-state auto
   vf 2 MAC 00:00:00:00:00:00, spoof checking on, link-state auto
   vf 3 MAC 00:00:00:00:00:00, spoof checking on, link-state auto
</code></pre></div></div>

<p>Setup the PF to be up</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ip link set enp3s0f0 up
</code></pre></div></div>

<p>Unbind the VFs from the driver</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>echo 0000:03:00.2 &gt; /sys/bus/pci/drivers/mlx5_core/unbind
echo 0000:03:00.3 &gt; /sys/bus/pci/drivers/mlx5_core/unbind
echo 0000:03:00.4 &gt; /sys/bus/pci/drivers/mlx5_core/unbind
echo 0000:03:00.5 &gt; /sys/bus/pci/drivers/mlx5_core/unbind
</code></pre></div></div>

<p>Configure SR-IOV VFs to switchdev mode</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>devlink dev eswitch set pci/0000:03:00.0 mode switchdev
ethtool -K enp3s0f0 hw-tc-offload on
</code></pre></div></div>

<p>Bind the VFs to the driver</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>echo 0000:03:00.2 &gt; /sys/bus/pci/drivers/mlx5_core/bind
echo 0000:03:00.3 &gt; /sys/bus/pci/drivers/mlx5_core/bind
echo 0000:03:00.4 &gt; /sys/bus/pci/drivers/mlx5_core/bind
echo 0000:03:00.5 &gt; /sys/bus/pci/drivers/mlx5_core/bind
</code></pre></div></div>

<p>Set hw-offload=true restart Open vSwitch</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>systemctl enable openvswitch.service
ovs-vsctl set Open_vSwitch . other_config:hw-offload=true
systemctl restart openvswitch.service
</code></pre></div></div>

<h2 id="worker-node-sr-iov-network-device-plugin-configuration">Worker Node SR-IOV network device plugin configuration</h2>

<p>This plugin creates device plugin endpoints based on the configurations given in file <code class="language-plaintext highlighter-rouge">/etc/pcidp/config.json</code>.
This configuration file is in json format as shown below:</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
    </span><span class="nl">"resourceList"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
         </span><span class="p">{</span><span class="w">
            </span><span class="nl">"resourceName"</span><span class="p">:</span><span class="w"> </span><span class="s2">"cx5_sriov_switchdev"</span><span class="p">,</span><span class="w">
            </span><span class="nl">"selectors"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
                </span><span class="nl">"vendors"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"15b3"</span><span class="p">],</span><span class="w">
                </span><span class="nl">"devices"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"1018"</span><span class="p">]</span><span class="w">
            </span><span class="p">}</span><span class="w">
        </span><span class="p">}</span><span class="w">
    </span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>Deploy SR-IOV network device plugin as daemonset see https://github.com/intel/sriov-network-device-plugin</p>

<h2 id="worker-node-multus-cni-configuration">Worker Node Multus CNI configuration</h2>

<p>Multus Config</p>
<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"multus-cni-network"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"multus"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"clusterNetwork"</span><span class="p">:</span><span class="w"> </span><span class="s2">"default"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"defaultNetworks"</span><span class="p">:[],</span><span class="w">
  </span><span class="nl">"kubeconfig"</span><span class="p">:</span><span class="w"> </span><span class="s2">"/etc/kubernetes/node-kubeconfig.yaml"</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>Deploy multus CNI as daemonset see https://github.com/intel/multus-cni</p>

<p>Create NetworkAttachementDefinition CRD with OVN CNI config</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">Kubernetes Network CRD Spec</span><span class="pi">:</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s2">"</span><span class="s">k8s.cni.cncf.io/v1"</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">NetworkAttachmentDefinition</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">default</span>
  <span class="na">annotations</span><span class="pi">:</span>
    <span class="na">k8s.v1.cni.cncf.io/resourceName</span><span class="pi">:</span> <span class="s">mellanox.com/cx5_sriov_switchdev</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">Config</span><span class="pi">:</span> <span class="s1">'</span><span class="s">{"cniVersion":"0.3.1","name":"ovn-kubernetes","type":"ovn-k8s-cni-overlay","ipam":{},"dns":{}}'</span>
</code></pre></div></div>

<h2 id="deploy-pod-with-ovs-hardware-offload">Deploy POD with OVS hardware-offload</h2>

<p>Create POD spec and</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Pod</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">ovs-offload-pod1</span>
  <span class="na">annotations</span><span class="pi">:</span>
    <span class="na">v1.multus-cni.io/default-network</span><span class="pi">:</span> <span class="s">default</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">containers</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">appcntr1</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">centos/tools</span>
    <span class="na">resources</span><span class="pi">:</span>
      <span class="na">requests</span><span class="pi">:</span>
        <span class="na">mellanox.com/cx5_sriov_switchdev</span><span class="pi">:</span> <span class="s1">'</span><span class="s">1'</span>
      <span class="na">limits</span><span class="pi">:</span>
        <span class="na">mellanox.com/cx5_sriov_switchdev</span><span class="pi">:</span> <span class="s1">'</span><span class="s">1'</span>
</code></pre></div></div>

<h2 id="verify-hardware-offload-is-working">Verify Hardware-Offload is working</h2>

<p>Lookup VF representor, in this example it is e5a1c8fcef0f327</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ ip link show enp3s0f0
6: enp3s0f0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq master ovs-system state UP mode DEFAULT group default qlen 1000
   link/ether ec:0d:9a:46:9e:84 brd ff:ff:ff:ff:ff:ff
   vf 0 MAC 00:00:00:00:00:00, spoof checking off, link-state enable, trust off, query_rss off
   vf 1 MAC 00:00:00:00:00:00, spoof checking off, link-state enable, trust off, query_rss off
   vf 2 MAC 00:00:00:00:00:00, spoof checking off, link-state enable, trust off, query_rss off
   vf 3 MAC fa:16:3e:b9:b8:ce, vlan 57, spoof checking on, link-state enable, trust off, query_rss off

compute_node2# ls -l /sys/class/net/
lrwxrwxrwx 1 root root 0 Sep 11 10:54 eth0 -&gt; ../../devices/virtual/net/eth0
lrwxrwxrwx 1 root root 0 Sep 11 10:54 eth1 -&gt; ../../devices/virtual/net/eth1
lrwxrwxrwx 1 root root 0 Sep 11 10:54 eth2 -&gt; ../../devices/virtual/net/eth2
lrwxrwxrwx 1 root root 0 Sep 11 10:54 e5a1c8fcef0f327 -&gt; ../../devices/virtual/net/e5a1c8fcef0f327
</code></pre></div></div>

<p>Access the POD</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl exec -it ovs-offload-pod1 -- /bin/bash
</code></pre></div></div>

<p>Ping other POD on second worker node</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ping ovs-offload-pod2
</code></pre></div></div>

<p>Check traffic on the VF representor port. Verify that only the first ICMP packet appears</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tcpdump -nnn -i e5a1c8fcef0f327

tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
17:12:41.260487 IP 172.0.0.13 &gt; 172.0.0.10: ICMP echo request, id 1263, seq 1, length 64
17:12:41.260778 IP 172.0.0.10 &gt; 172.0.0.13: ICMP echo reply, id 1263, seq 1, length 64
17:12:46.268951 ARP, Request who-has 172.0.0.13 tell 172.0.0.10, length 42
17:12:46.271771 ARP, Reply 172.0.0.13 is-at fa:16:3e:1a:10:05, length 46
17:12:55.354737 IP6 fe80::f816:3eff:fe29:8118 &gt; ff02::1: ICMP6, router advertisement, length 64
17:12:56.106705 IP 0.0.0.0.68 &gt; 255.255.255.255.67: BOOTP/DHCP, Request from 62:21:f0:89:40:73, length 30
</code></pre></div></div>

<h2 id="ovs-hardware-offload-dpu-support">OVS hardware offload DPU support</h2>

<p><a href="https://blogs.nvidia.com/blog/2020/05/20/whats-a-dpu-data-processing-unit/">Data Processing Units</a> (DPU) combine the advanced capabilities
of a Smart-NIC (such as Mellanox ConnectX-6DX NIC) with a general purpose embedded CPU and a high-speed memory controller.</p>

<p>Similarly to Smart-NICs, a DPU follows the kernel switchdev model.
In this model, every VF/PF net-device on the host has a corresponding representor net-device existing on
the embedded CPU.</p>

<h3 id="supported-dpus">Supported DPUs</h3>

<p>The following manufacturers are known to work:</p>

<ul>
  <li><a href="https://www.mellanox.com/products/bluefield2-overview">Mellanox Bluefield-2</a></li>
</ul>

<p>Deployment guide can be found <a href="https://docs.google.com/document/d/1hRke0cOCY84Ef8OU283iPg_PHiJ6O17aUkb9Vv-fWPQ/edit?usp=sharing">here</a>.</p>

<h2 id="vdpa">vDPA</h2>

<p>vDPA (Virtio DataPath Acceleration) is a technology that enables the acceleration of virtIO devices while
allowing the implementations of such devices (e.g: NIC vendors) to use their own control plane.</p>

<p>vDPA can be combined with the SR-IOV OVS Hardware offloading setup to expose the workload to an
open standard interface such as virtio-net.</p>

<h3 id="additional-prerequisites">Additional Prerequisites:</h3>
<ul>
  <li>Linux Kernel &gt;= 5.12</li>
  <li>iproute &gt;= 5.14</li>
</ul>

<h3 id="supported-hardware">Supported Hardware:</h3>
<ul>
  <li>Mellanox ConnectX-6DX NIC</li>
</ul>

<h3 id="additional-configuration">Additional configuration</h3>
<p>In addition to all the steps listed above, insert the virtio-vdpa driver and the mlx-vdpa driver:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ modprobe vdpa
$ modprobe virtio-vdpa
$ modprobe mlx5-vdpa
</code></pre></div></div>

<p>The the <code class="language-plaintext highlighter-rouge">vdpa</code> tool (part of iproute package) is used to create a vdpa device on top
of an existing VF:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ vdpa mgmtdev show
pci/0000:65:00.2:
  supported_classes net
$ vdpa dev add name vdpa2 mgmtdev pci/0000:65:00.2
$ vdpa dev list
vdpa2: type network mgmtdev pci/0000:65:00.2 vendor_id 5555 max_vqs 16 max_vq_size 256
</code></pre></div></div>

<p>After a device has been created, the SR-IOV Device Plugin plugin configuration has to be modified for it
to select and expose the vdpa device:</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
    </span><span class="nl">"resourceList"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
         </span><span class="p">{</span><span class="w">
            </span><span class="nl">"resourceName"</span><span class="p">:</span><span class="w"> </span><span class="s2">"cx6_sriov_vpda_virtio"</span><span class="p">,</span><span class="w">
            </span><span class="nl">"selectors"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
               </span><span class="nl">"vendors"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"15b3"</span><span class="p">],</span><span class="w">
               </span><span class="nl">"devices"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"101e"</span><span class="p">],</span><span class="w">
               </span><span class="nl">"vdpaType"</span><span class="p">:</span><span class="w"> </span><span class="s2">"virtio"</span><span class="w">
            </span><span class="p">}</span><span class="w">
        </span><span class="p">}</span><span class="w">
    </span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

        </section>

        <aside id="sidebar">
          

          
            <p class="repo-owner"><a href="https://github.com/ovn-org/ovn-kubernetes">ovn-kubernetes</a> is maintained by <a href="https://github.com/ovn-org">ovn-org</a>.</p>
          

          <p>This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</p>
        </aside>
      </div>
    </div>

  </body>
</html>
